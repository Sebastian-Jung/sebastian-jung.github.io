<!DOCTYPE html>
<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V4WY96CK58"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-V4WY96CK58');
</script>
<meta name="description" content="Finding NeMO: A Geometry-Aware Representation of Template Views for Few-Shot Perception. Code, paper, and examples for 3D object perception.">
<meta name="keywords" content="3D vision, point cloud, few-shot learning, NeMO, computer vision">
<meta name="author" content="Sebastian Jung">
<meta name="robots" content="index, follow">
<meta charset="UTF-8">
<title>NeMO</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="style.css">
<style>
body { margin:0; font-family:'Inter', sans-serif; background:#fafafa; color:#333; }
.navbar { background:white; padding:20px 40px; box-shadow:0 2px 4px rgba(0,0,0,0.1); position:sticky; top:0; z-index:100; }
.header { text-align:center; margin:60px 0 40px; }
.title { font-size:40px; font-weight:700; }
.authors { font-size:20px; color:#555; }
.links a { margin:0 10px; font-weight:600; text-decoration:none; color:#0066cc; }
.section { max-width:900px; margin:40px auto; padding:0 20px; }
h2 { font-size:32px; font-weight:700; margin-bottom:15px; }
#teaser { width:100%; border-radius:12px; box-shadow:0 4px 10px rgba(0,0,0,0.1); }
#architecture { width:100%;}
.citation-box { background:#f0f0f0; padding:20px; border-radius:12px; font-family:monospace; }
.carousel-container { position:relative; width:100%; max-width:900px; height:500px; overflow:hidden; border-radius:12px; background:#FFFFFF; margin:20px auto; }
.carousel-track { display:flex; height:100%; transition:transform 0.5s ease; }
.viewer-wrapper { flex:0 0 100%; height:100%; }
.arrow { position:absolute; top:0; width:50px; height:100%; background:rgba(128,128,128,0.3); border:none; cursor:pointer; font-size:40px; font-weight:bold; color:#fff; display:flex; align-items:center; justify-content:center; z-index:10; }
.arrow:hover { background:rgba(255,255,255,0.5); }
#prev { left:0; }
#next { right:0; }
footer { text-align:center; padding:20px; margin-top:50px; color:#aaa; }
</style>
</head>
<body>

<div class="header">
  <div class="title">Finding NeMO üê†</div>
  <div class="subtitle">A Geometry-Aware Representation of Template Views for Few-Shot Perception</div>
  <div class="banner">üèÜ 3DV 2026 ‚Äî Oral Presentation</div>
  <br><br>
  <div class="authors"><a href="../index.html">Sebastian Jung</a>, Leonard Kl√ºpfel, Rudolph Triebel, Maximilian Durner</div>
  <div class="links"><a href="paper.pdf">üìÑ Paper</a><a href="#">üìù arXiv</a><a href="https://github.com/DLR-RM/nemo">üíª Code</a><a href="#results">üìä Results</a></div>
</div>

<div class="section">
  <img id="teaser" src="assets/teaser.png" alt="Teaser image">
</div>

<div class="section" id="abstract">
  <h2>Abstract</h2>
  <p>We present <b>Neural Memory Object (NeMO)</b>, a novel object-centric representation that can be used to detect, segment and estimate the 6DoF pose of objects unseen during training using RGB images. Our method consists of an encoder that requires only a few RGB template views depicting an object to generate a sparse object-like point cloud using a learned UDF containing semantic and geometric information. Next, a decoder takes the object encoding together with a query image to generate a variety of dense predictions. Through extensive experiments, we show that our method can be used for few-shot object perception without requiring any camera-specific parameters or retraining on target data. Our proposed concept of outsourcing object information in a NeMO and using a single network for multiple perception tasks enhances interaction with novel objects, improving scalability and efficiency by enabling quick object onboarding without retraining or extensive pre-processing. We report competitive and state-of-the-art results on various datasets and perception tasks of the BOP benchmark, demonstrating the versatility of our approach. Code and synthetic dataset will be released.</p>
</div>


<div class="section" id="neural-memory-objects">
  <h2>Neural Memory Objects</h2>
  <p>Our novel encoder combines the information stored in a set of images into a unified, geometrically understandable representation. The encoder learns to predict the objects geometry as a 3D point cloud and associates semantic features with each point. We jointly learn a decoder that can use the NeMO encoding to predict an object centric pointmap in the coordinate system of the NeMO encoding, a modal and an amodal semantic mask.</p>
  <img id="architecture" src="assets/architecture.png" alt="Architecture image">
</div>

<div class="section" id="pc-section">
  <h2>Surface and Pose Estimation</h2>
  <p>Using the pointmap predictions of the NeMO Decoder, we can estimate object-centric surfaces and camera poses using only a few images. Our method is able to robustly estimate camera poses and object surfaces in various settings - A static object and a moving camera, a static camera and an object rotated in hand and both moving cameras and the object being in different scenarios, including occlusion. We also show the estimated positions of the cameras relative to the object (not to scale):</p>
  <div class="carousel-container">
    <div class="carousel-track" id="carouselTrack">
      <div class="viewer-wrapper" id="viewer1"></div>
      <div class="viewer-wrapper" id="viewer2"></div>
      <div class="viewer-wrapper" id="viewer3"></div>
    </div>
    <button class="arrow" id="prev">‚Äπ</button>
    <button class="arrow" id="next">‚Ä∫</button>
  </div>
</div>

<script type="module">
import { PointCloudViewer } from './PointCloudViewer.js';

const configs = [
  { id:'viewer1', json:'./nemos/coffee_machine/points.json', images:['./nemos/coffee_machine/object0_0.jpg','./nemos/coffee_machine/object0_1.jpg','./nemos/coffee_machine/object0_2.jpg','./nemos/coffee_machine/object0_3.jpg'] },
  { id:'viewer2', json:'./nemos/esspresso_mug_in_hand/points.json', images:['./nemos/esspresso_mug_in_hand/object0_0.jpg','./nemos/esspresso_mug_in_hand/object0_1.jpg','./nemos/esspresso_mug_in_hand/object0_2.jpg','./nemos/esspresso_mug_in_hand/object0_3.jpg'] },
  { id:'viewer3', json:'./nemos/label_machine/points.json', images:['./nemos/label_machine/object0_0.jpg','./nemos/label_machine/object0_1.jpg','./nemos/label_machine/object0_2.jpg','./nemos/label_machine/object0_3.jpg'] },
];

const viewers = configs.map(c => new PointCloudViewer(document.getElementById(c.id), c.json, c.images));

const track = document.getElementById('carouselTrack');
let currentIndex = 0;

function updateCarousel() {
  const offset = -currentIndex * 100;
  track.style.transform = `translateX(${offset}%)`;
  viewers[currentIndex].resizeAndRender();
}

document.getElementById('prev').onclick = () => { if(currentIndex > 0){ currentIndex--; updateCarousel(); } };
document.getElementById('next').onclick = () => { if(currentIndex < viewers.length-1){ currentIndex++; updateCarousel(); } };

updateCarousel();
</script>

<div class="section" id="segmentation">
  <h2>Segmentation</h2>
  <p>The decoder learns to predict modal and amodal object segmentation. We can predict amodal segmentation masks of new objects that have never been seen during training.</p>
</div>

<div class="section">
  <h2>BibTeX</h2>
  <div class="citation-box">@inproceedings{jung2026,title={Finding NeMO: A Geometry-Aware Representation of Template Views for Few-Shot Perception},author={Sebastian Jung and Leonard Kl√ºpfel and Rudolph Triebel and Maximilian Durner},journal={3DV},year={2026}}</div>
</div>

<footer>¬© 2025 Sebastian Jung ‚Äî Finding NeMO</footer>

</body>
</html>
